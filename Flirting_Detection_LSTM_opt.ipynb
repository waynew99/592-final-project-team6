{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waynew99/592-final-project-team6/blob/main/Flirting_Detection_LSTM_opt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYJq23RCXUdh"
      },
      "source": [
        "# Imports & Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTnfBbvzW_a8",
        "outputId": "c65cb2ab-4153-4ec1-c19b-1d83f2b7465f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "print(np.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rydX9lONYSkH",
        "outputId": "8825f29e-8f52-48d5-b6d2-2aea648b6643"
      },
      "outputs": [],
      "source": [
        "! pip install datasets\n",
        "! pip install codecarbon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmRgzs9lZdUt"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL-S-2_CXhtH",
        "outputId": "eb412d88-25f4-463f-8967-202a25b0483c"
      },
      "outputs": [],
      "source": [
        "# Dataset 1 (from hugging face)\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets, ClassLabel, Features\n",
        "from codecarbon import EmissionsTracker\n",
        "dataset = load_dataset(\"ieuniversity/flirty_or_not\")\n",
        "dataset = dataset.remove_columns('id')\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "train_dataset = dataset['train']\n",
        "validation_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZCSdCdCoDTO",
        "outputId": "0fda87ff-6182-4125-ef61-9fe3589b0de6"
      },
      "outputs": [],
      "source": [
        "# Dataset 2 (from local upload)\n",
        "data = pd.read_csv('flirting_rated.csv')\n",
        "data.drop_duplicates(subset=['texts'], inplace=True)\n",
        "data.dropna(subset=['label'], inplace=True)\n",
        "dataset_2 = Dataset.from_pandas(data)\n",
        "print(len(dataset_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWa0Vbl5pVZ8",
        "outputId": "7edbe467-7014-4e6f-daaa-3cd3220e55e5"
      },
      "outputs": [],
      "source": [
        "split_train_dataset = dataset_2.train_test_split(test_size=0.2)['train']\n",
        "split_valid_dataset = dataset_2.train_test_split(test_size=0.2)['test'].train_test_split(test_size=0.5)['train']\n",
        "split_test_dataset = dataset_2.train_test_split(test_size=0.2)['test'].train_test_split(test_size=0.5)['test']\n",
        "\n",
        "print(len(split_train_dataset))\n",
        "print(len(split_valid_dataset))\n",
        "print(len(split_test_dataset))\n",
        "\n",
        "class_label = ClassLabel(names=['neutral', 'flirty'])\n",
        "\n",
        "# Define features for the dataset\n",
        "features = Features({\n",
        "    'label': class_label,\n",
        "    'texts': split_train_dataset.features['texts']\n",
        "})\n",
        "\n",
        "split_train_dataset = Dataset.from_dict({\n",
        "    'label': split_train_dataset['label'],\n",
        "    'texts': split_train_dataset['texts']\n",
        "}, features=features)\n",
        "\n",
        "split_valid_dataset = Dataset.from_dict({\n",
        "    'label': split_valid_dataset['label'],\n",
        "    'texts': split_valid_dataset['texts']\n",
        "}, features=features)\n",
        "\n",
        "split_test_dataset = Dataset.from_dict({\n",
        "    'label': split_test_dataset['label'],\n",
        "    'texts': split_test_dataset['texts']\n",
        "}, features=features)\n",
        "\n",
        "\n",
        "print(split_train_dataset.features)\n",
        "print(train_dataset.features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEMaG6bxY_-d",
        "outputId": "e86902dd-f6f3-473a-c94f-d2c428dc38a5"
      },
      "outputs": [],
      "source": [
        "# The second dataset is NOT balanced: Majority is neutral (0)\n",
        "total_flirty = 0\n",
        "for label in split_train_dataset['label']:\n",
        "  if label != 0:\n",
        "    total_flirty += 1\n",
        "\n",
        "for label in split_valid_dataset['label']:\n",
        "  if label != 0:\n",
        "    total_flirty += 1\n",
        "\n",
        "for label in split_test_dataset['label']:\n",
        "  if label != 0:\n",
        "    total_flirty += 1\n",
        "print(total_flirty/2659)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B5Hl3_gpuGP"
      },
      "outputs": [],
      "source": [
        "train_dataset_final = concatenate_datasets([split_train_dataset, train_dataset])\n",
        "validation_dataset_final = concatenate_datasets([validation_dataset, split_valid_dataset])\n",
        "test_dataset_final = concatenate_datasets([test_dataset, split_test_dataset])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4QaZACKDouU",
        "outputId": "2edf149a-66c0-4af1-b60c-18b1618338b6"
      },
      "outputs": [],
      "source": [
        "TRAIN_SIZE = len(train_dataset_final)\n",
        "VALIDATION_SIZE = len(validation_dataset_final)\n",
        "TEST_SIZE = len(test_dataset_final)\n",
        "print(\"Train: \", TRAIN_SIZE, \"Validation: \", VALIDATION_SIZE, \"Test: \", TEST_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_L4YxA5ciIi",
        "outputId": "88f73dad-d2cc-4130-b7b5-8aa9f29606b0"
      },
      "outputs": [],
      "source": [
        "total_length = 0\n",
        "total_samples = 5000\n",
        "\n",
        "for sample in train_dataset_final['texts']:\n",
        "    # print(sample)\n",
        "    # Calculate the length of each data instance\n",
        "    # Assuming each data instance is a dictionary-like object with a 'text' key\n",
        "    words = sample.split()\n",
        "    length = len(words)\n",
        "    total_length += length\n",
        "\n",
        "for sample in validation_dataset_final['texts']:\n",
        "    # print(sample)\n",
        "    # Calculate the length of each data instance\n",
        "    # Assuming each data instance is a dictionary-like object with a 'text' key\n",
        "    words = sample.split()\n",
        "    length = len(words)\n",
        "    total_length += length\n",
        "\n",
        "for sample in test_dataset_final['texts']:\n",
        "    # print(sample)\n",
        "    # Calculate the length of each data instance\n",
        "    # Assuming each data instance is a dictionary-like object with a 'text' key\n",
        "    words = sample.split()\n",
        "    length = len(words)\n",
        "    total_length += length\n",
        "\n",
        "# Compute the average length\n",
        "average_length = total_length / total_samples\n",
        "\n",
        "print(\"Average length of each data instance:\", average_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsIHEwxFXugT",
        "outputId": "8337d3fc-b9a7-4955-f4c5-0e4f11db41b8"
      },
      "outputs": [],
      "source": [
        "total_flirty = 0\n",
        "for label in train_dataset_final['label']:\n",
        "  total_flirty += label\n",
        "print(total_flirty/TRAIN_SIZE)\n",
        "\n",
        "total_flirty = 0\n",
        "for label in validation_dataset_final['label']:\n",
        "  total_flirty += label\n",
        "print(total_flirty/VALIDATION_SIZE)\n",
        "\n",
        "total_flirty = 0\n",
        "for label in test_dataset_final['label']:\n",
        "  total_flirty += label\n",
        "print(total_flirty/VALIDATION_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnnB4A4cxSNl"
      },
      "source": [
        "# Preprocess Dataset: Converting Text Data into Vectors/Numbers\n",
        "Models cannot directly process raw text, so we need to convert the text into numbers using TensorFlow's `TextVectorization` layer. Specifically, we can perform the following 3 operations all at once, by constructing this layer and feeding the data into it.\n",
        "\n",
        "* Standardization: preprocessing the text, typically to change all text to lowercase and remove punctuation to simplify the dataset.\n",
        "* Tokenization: dividing text into individual words called tokens.\n",
        "* Vectorization: converting tokens into numbers so they can be fed into a neural network.\n",
        "\n",
        "Constructing the layer is very easy (we simply call the function), but there are some hyperparameters we need to determine first.\n",
        "\n",
        "## TextVectorization\n",
        "First, *maximum vocabulary size* and *maximum sequence length* are 2 hyperparameters defined based on the nature of the dataset and the memory constraints of the machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6P1C6u4zTAw"
      },
      "source": [
        "### Maximum Vocabulary Size (`max_tokens`)\n",
        "\n",
        "This parameter determines the number of unique words that are considered when vectorizing texts. To decide this:\n",
        "\n",
        "*   Tokenize your dataset to find out the total number of unique tokens it contains.\n",
        "*   Evaluate your hardware's memory constraints since a larger vocabulary will require more memory.\n",
        "\n",
        "Typically, a number between 10,000 to 100,000 works well for many tasks, but if your dataset is very specialized, smaller might be enough. Also, it's often best to choose a number that is slightly above the number of unique tokens actually observed in your data to account for uncertainties. However, if the vocabulary size is significantly higher than the actual number of unique tokens, it can lead to increased memory overhead, slower training speed, and most importantly, possibility of overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWha4bUlxZn9"
      },
      "outputs": [],
      "source": [
        "# access the text and labels after the dataset is loaded successfully\n",
        "texts = [row['texts'] for row in train_dataset_final]\n",
        "labels = [row['label'] for row in train_dataset_final]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNS8kNsMx-Vq",
        "outputId": "86c23012-56fb-42ae-a747-35413846d03d"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of unique tokens using TensorFlow's tokenizer\n",
        "from collections import Counter\n",
        "\n",
        "# Flatten the list of sentences into a single list of words\n",
        "all_words = [word for text in texts for word in text.split()]\n",
        "\n",
        "# Count the unique words\n",
        "word_count = Counter(all_words)\n",
        "# ~4000 unique words/tokens in the training + validation set\n",
        "print(f\"Total unique tokens in the raw text: {len(word_count)}\")\n",
        "\n",
        "# Thus, probably use about 4000-5000 for token size\n",
        "MAX_FEATURES = 5000  # Size of the vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ph3ylNQzup2"
      },
      "source": [
        "### Maxmimum Sequence Length (`output_sequence_length`)\n",
        "\n",
        "This parameter controls the maximum number of tokens that will be considered in each example. To find a reasonable maximum sequence length:\n",
        "\n",
        "- Calculate the length of each text example in your dataset.\n",
        "- Plot a histogram of these lengths to get a sense of the distribution.\n",
        "- Use this information to decide on a length that covers most of your texts without being excessively long.\n",
        "\n",
        "The `output_sequence_length` parameter is used to pad or truncate sequences to this maximum length. If a text is shorter than this, it will be padded with zeros, and if it's longer, it will be truncated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "fp15Zvlx5jGX",
        "outputId": "46009bb3-2800-4390-c1e9-72c581c09b24"
      },
      "outputs": [],
      "source": [
        "# Find a reasonable sequence length\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the length for each piece of text\n",
        "text_lengths = [len(text.split()) for text in texts]\n",
        "\n",
        "# Plot a histogram\n",
        "plt.hist(text_lengths, bins=50)\n",
        "plt.xlabel('Length of Texts')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# 5 number summary showing distribution of lengths\n",
        "# calculate quartiles\n",
        "quartiles = np.percentile(text_lengths, [25, 50, 75])\n",
        "# calculate min/max\n",
        "len_min, len_max = min(text_lengths), max(text_lengths)\n",
        "# print 5-number summary\n",
        "print('Min: %.3f' % len_min)\n",
        "print('Q1: %.3f' % quartiles[0])\n",
        "print('Median: %.3f' % quartiles[1])\n",
        "print('Q3: %.3f' % quartiles[2])\n",
        "print('Max: %.3f' % len_max)\n",
        "\n",
        "# Determine a suitable maximum length (e.g., the 95th percentile could be a good starting point)\n",
        "SEQUENCE_LENGTH = int(np.percentile(text_lengths, 95)) # Length of the input sequences\n",
        "SEQUENCE_LENGTH = 24\n",
        "print(SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGNrBjqJ04uD"
      },
      "source": [
        "### Output Mode and Standardization\n",
        "\n",
        "These are 2 more hyperparameters for the `TextVectorization` layer.\n",
        "\n",
        "First, for output mode, we are setting it to `int`. As a result, the layer converts the tokens into integer indices. Each unique token is assigned a specific integer value, and texts are converted into sequences of these integers. In many deep learning models for text based on neural networks, `int` is the preferred option because it works very well with Embedding Layers, which are efficient and powerful mechanisms for handling sequences of tokens.\n",
        "\n",
        "Next, for standardization, we are currently just changing each token into lowercase, a very common standardize function. We are *not* removing punctuation and special characters because there may be special characters (e.g. emojis) that actually contribute to the meaning of the text, so removing them may reduce context.\n",
        "\n",
        "As a side note, we do not need to further define a custome split hyperparameter because splitting by whitespace (default) is sufficient for our case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyCnDT5eHRrp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import keras\n",
        "\n",
        "# @keras.saving.register_keras_serializable()\n",
        "# def custom_standardization(text):\n",
        "#   # Make text lowercase\n",
        "#   text = tf.strings.lower(text)\n",
        "#   # Only remove common punctuation\n",
        "#   # common_punct = r'[,.?!]'\n",
        "#   common_punct = r\"[,.?!\\'\\\"\\<\\>\\{\\}\\[\\]\\^\\&\\\\\\%\\$\\#\\@\\|]\"\n",
        "#   text = tf.strings.regex_replace(text, common_punct, '')\n",
        "#   return text\n",
        "\n",
        "# Define the function that will treat punctuation as tokens?\n",
        "@keras.saving.register_keras_serializable()\n",
        "def custom_standardization(text):\n",
        "  # Make text lowercase\n",
        "  text = tf.strings.lower(text)\n",
        "  # Remove insignificant punctuation from the text\n",
        "  common_punct = r\"[,\\'\\\"\\<\\>\\{\\}\\[\\]\\^\\&\\\\\\%\\$\\#\\@\\|]\"\n",
        "  text = tf.strings.regex_replace(text, common_punct, '')\n",
        "  # Replace punctuations with space + punctuation + space\n",
        "  for punct in string.punctuation:\n",
        "      text = tf.strings.regex_replace(text, re.escape(punct), f\" {punct} \")\n",
        "  # Remove extra spaces\n",
        "  text = tf.strings.strip(text)\n",
        "  text = tf.strings.regex_replace(text, ' +', ' ')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZMtWxPf63Fj"
      },
      "outputs": [],
      "source": [
        "# tokenization & vectorization\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=MAX_FEATURES,\n",
        "    standardize=custom_standardization, # lowercase & remove common punctuation\n",
        "    output_mode='int', # int is preferred mode in modern day DL tasks\n",
        "    output_sequence_length=SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKcBZi8p9I0e",
        "outputId": "dc7a5d68-dfd2-4531-af16-cbca1daf5d88"
      },
      "outputs": [],
      "source": [
        "# adapt the TextVectorization layer to text data so that it can build the vocabulary\n",
        "vectorize_layer.adapt(texts)\n",
        "# test\n",
        "len(vectorize_layer.get_vocabulary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P05mlYs9Uiot"
      },
      "outputs": [],
      "source": [
        "# # Let's prepare a dataset to illustrate the functionality\n",
        "# example_dataset = tf.data.Dataset.from_tensor_slices([\"Hello!\", \"Hello !\", \"It's a? test ðŸ˜‰?\", \"It's a test :) :(\"])\n",
        "# vectorize_layer.adapt(example_dataset)\n",
        "\n",
        "# print(vectorize_layer.get_vocabulary())\n",
        "\n",
        "# # Example of how the layer will preprocess the data\n",
        "# for example in example_dataset:\n",
        "#     print(\"Original:\", example.numpy())\n",
        "#     vectorized_text = vectorize_layer(example)\n",
        "#     print(\"Vectorized:\", vectorized_text.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egW9ZW6e5fWc"
      },
      "source": [
        "## Convert to Tensorflow Dataset (for training)\n",
        "\n",
        "Now, we can convert the raw text data into a form that a TensorFlow machine learning model can understand (i.e., numeric tensors) using the TextVectorization layer, preparing it in batches to be fed into the model for training or inference.\n",
        "\n",
        "We can either (1) do the convertion as part of data preprocessing, or (2) leave it to the LSTM model itself. Option (2) generalizes better if we want to deploy and test the model since it can automatically convert text into numbers without any manual preprocessing, so we are going with this option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSx1YxVv7qmV"
      },
      "outputs": [],
      "source": [
        "# another hyperparameter\n",
        "BATCH_SIZE = 32\n",
        "# takes text data (a list of strings) and creates Dataset object\n",
        "# each element of the dataset is one piece of text from your original list\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(texts).batch(BATCH_SIZE)\n",
        "# Turn the texts into sequences of integers [Option 1]\n",
        "# text_int_sequences = text_ds.map(vectorize_layer)\n",
        "# convert corresponding labels to tensorflow format\n",
        "label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(labels, tf.int32)).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VK6aKF6vAgN4"
      },
      "outputs": [],
      "source": [
        "# integer representing the number of elements to shuffle at a time\n",
        "shuffle_buffer_size = TRAIN_SIZE # the number of data in the training set\n",
        "# build the training dataset & optimize it for training\n",
        "# train_ds = tf.data.Dataset.zip((text_int_sequences, label_ds)) \\ # [Option 1]\n",
        "train_ds = tf.data.Dataset.zip((text_ds, label_ds)) \\\n",
        "            .shuffle(buffer_size=shuffle_buffer_size) \\\n",
        "            .cache() \\\n",
        "            .prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEWeTi5G_Bgc"
      },
      "source": [
        "### Convert Validation Set and Test Set as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J2OsSjFBdhB"
      },
      "outputs": [],
      "source": [
        "val_texts = [row['texts'] for row in validation_dataset_final]\n",
        "val_labels = [row['label'] for row in validation_dataset_final]\n",
        "val_text_ds = tf.data.Dataset.from_tensor_slices(val_texts).batch(BATCH_SIZE)\n",
        "val_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(val_labels, tf.int32)).batch(BATCH_SIZE)\n",
        "# val_text_int_sequences = val_text_ds.map(vectorize_layer) # [Option 1]\n",
        "# note: no shuffling for validation and test sets\n",
        "val_ds = tf.data.Dataset.zip((val_text_ds, val_label_ds)) \\\n",
        "            .cache() \\\n",
        "            .prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwfkPN1bXdL6"
      },
      "outputs": [],
      "source": [
        "test_texts = [row['texts'] for row in test_dataset_final]\n",
        "test_labels = [row['label'] for row in test_dataset_final]\n",
        "test_text_ds = tf.data.Dataset.from_tensor_slices(test_texts).batch(BATCH_SIZE)\n",
        "test_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(test_labels, tf.int32)).batch(BATCH_SIZE)\n",
        "test_ds = tf.data.Dataset.zip((test_text_ds, test_label_ds)) \\\n",
        "            .cache() \\\n",
        "            .prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu8qKSswAk6S"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "# for example, label in val_ds.take(1):\n",
        "#   print('texts: ', example.numpy())\n",
        "#   print()\n",
        "#   print('labels: ', label.numpy())\n",
        "# exact_sample_count = sum(1 for _ in train_ds.unbatch())  # This can be slow for large datasets\n",
        "# print(\"Total number of samples in train_ds:\", exact_sample_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XovGLmnyHmYK"
      },
      "source": [
        "# Build LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-3Q5n1sHqst"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# More Hyperparameters\n",
        "EMBEDDING_DIM = 128    # Dimension of the embedding vectors\n",
        "LSTM_UNITS = 16        # The number of units in the LSTM layer (x2 if bidirectional is specified)\n",
        "DROPOUT_RATE = 0.25\n",
        "REGULAR_RATE = 0.001\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    # [Option 2] convert text to vectors\n",
        "    vectorize_layer,\n",
        "    # TODO: can switch the embedding to be a pre-trained model, e.g. word2vec\n",
        "    layers.Embedding(\n",
        "        input_dim=MAX_FEATURES + 1,\n",
        "        # input_dim=len(vectorize_layer.get_vocabulary()) + 1,\n",
        "        output_dim=EMBEDDING_DIM,\n",
        "        input_length=SEQUENCE_LENGTH,\n",
        "        mask_zero=True),\n",
        "    # Avoid overfitting\n",
        "    layers.SpatialDropout1D(DROPOUT_RATE),\n",
        "    # LSTM layer\n",
        "    layers.Bidirectional(layers.LSTM(\n",
        "        LSTM_UNITS,\n",
        "        dropout=DROPOUT_RATE,\n",
        "        recurrent_dropout=DROPOUT_RATE,\n",
        "        return_sequences=True)),\n",
        "    layers.Bidirectional(layers.LSTM(\n",
        "        LSTM_UNITS,\n",
        "        dropout=DROPOUT_RATE,\n",
        "        recurrent_dropout=DROPOUT_RATE)),\n",
        "    # layers.LSTM(\n",
        "    #     LSTM_UNITS,\n",
        "    #     dropout=DROPOUT_RATE,\n",
        "    #     recurrent_dropout=DROPOUT_RATE,\n",
        "    #     kernel_regularizer=regularizers.l2(REGULAR_RATE),\n",
        "    #     recurrent_regularizer=regularizers.l2(REGULAR_RATE)),\n",
        "    # increasing complexity\n",
        "    # layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(REGULAR_RATE)),\n",
        "    # using sigmoid activation function for binary classification problem\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoUjjpphDFfR",
        "outputId": "8b0bb500-a8fc-4ffd-9d81-c836c2f78e0d"
      },
      "outputs": [],
      "source": [
        "print(MAX_FEATURES)\n",
        "print(SEQUENCE_LENGTH)\n",
        "print(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIEGHofbRFfM",
        "outputId": "88234b39-73ab-4934-e52c-de77d059a971"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zL_1LhvVpjN"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj_C98AuhRZF"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=3,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True  # Restores model weights from the epoch with the minimum validation loss.\n",
        ")\n",
        "\n",
        "# Save model checkpoints\n",
        "checkpoint_filepath = 'training/best_model.h5'\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "# Reduce learning rate on plateau\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.1,\n",
        "    patience=3,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6_0YR3eVrTU",
        "outputId": "78d3570f-9ceb-4dde-a994-86ced726c239"
      },
      "outputs": [],
      "source": [
        "# Even More Hyperparameters\n",
        "EPOCHS = 10\n",
        "\n",
        "history = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=[early_stopping, reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTbAGTR8W8ip",
        "outputId": "48c8f1e9-ff02-4a7f-f757-94bf43614fd5"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs1qKXL3YLcQ",
        "outputId": "b35eef98-70e4-49b3-bc98-9a8842e71639"
      },
      "outputs": [],
      "source": [
        "sample_text = ('Lets explore each others body')\n",
        "tracker = EmissionsTracker()\n",
        "tracker.start()\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "emissions: float = tracker.stop()\n",
        "print(f\"Emissions: {emissions} kg\")\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edd6Nby7QQfn"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVc1-hjYeciS"
      },
      "source": [
        "## Training Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFqVO_UPQSJO",
        "outputId": "da456670-ead4-4375-cf53-f387916b10f0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "history_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuvVrxn9Q-r1"
      },
      "outputs": [],
      "source": [
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "epochs = range(1, len(acc) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "PEe3s5KfQaZt",
        "outputId": "019558f7-16be-4d52-c319-705b3f5500fa"
      },
      "outputs": [],
      "source": [
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
        "plt.title(f'Training and Validation Loss over {EPOCHS} Epochs of LSTM Model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "wf9MqgY9SNLD",
        "outputId": "94fc4823-3dab-4b3d-bd71-2135360c81be"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, acc, 'bo-', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'ro-', label='Validation acc')\n",
        "plt.title(f'Training and Validation Accuracy over {EPOCHS} Epochs of LSTM Model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXHNM2DXeYSo"
      },
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmmZVVjLTIHz",
        "outputId": "9fc7cf6d-3009-48b2-9386-ee750774242b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# make predictions on test set\n",
        "y_pred = model.predict(test_ds)\n",
        "# convert these probabilities to binary predictions\n",
        "# e.g., classify samples with a probability > 0.5 as positive (flirty)\n",
        "y_pred_binary = (y_pred > 0.5).astype(\"int32\")\n",
        "# Prepare the true labels. You'll need to concatenate them into one array.\n",
        "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "iieDzNFUWeJD",
        "outputId": "81958088-19e2-4d7d-b9d5-0d8c2251e47a"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# plot the confusion matrix using seaborn (for better visualization)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title(\"Confusion Matrix for LSTM Model's Prediction on Test Set\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf6cYbp9efD_"
      },
      "source": [
        "## See Wrong Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyV3R8-7XOnH"
      },
      "outputs": [],
      "source": [
        "# Get binary prediction produced by model\n",
        "y_pred1 = (y_pred > 0.5).astype(\"int32\").flatten()\n",
        "# Collect true labels\n",
        "y_true1 = np.concatenate([labels.numpy() for inputs, labels in test_ds]).flatten()\n",
        "# Collect input data (assuming the input data is a NumPy array or a list)\n",
        "x_test1 = np.concatenate([inputs.numpy() for inputs, labels in test_ds]).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIWObqEmbh61"
      },
      "outputs": [],
      "source": [
        "# Identify indices of incorrect predictions\n",
        "false_positives = np.where((y_pred1 == 1) & (y_true1 == 0))[0]\n",
        "false_negatives = np.where((y_pred1 == 0) & (y_true1 == 1))[0]\n",
        "# Retrieve the corresponding input text for false positives and false negatives\n",
        "fp_texts = [x_test1[i] for i in false_positives]\n",
        "fn_texts = [x_test1[i] for i in false_negatives]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8lUXX_Wb4f6",
        "outputId": "450b713e-e6c3-46e1-edc1-54cbfb23e0ef"
      },
      "outputs": [],
      "source": [
        "# Now, if you want to visualize some of them:\n",
        "print(\"False Positives:\")\n",
        "for i, fp_idx in enumerate(false_positives):  # Show first 10 false positives\n",
        "  print(f\"Text: {fp_texts[i]} - Predicted: {y_pred[fp_idx]}, Actual: {y_true[fp_idx]}\")\n",
        "print(\"\\n\")\n",
        "print(\"False Negatives:\")\n",
        "for i, fn_idx in enumerate(false_negatives):  # Show first 10 false negatives\n",
        "  print(f\"Text: {fn_texts[i]} - Predicted: {y_pred[fn_idx]}, Actual: {y_true[fn_idx]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C_tVqAXemwC"
      },
      "source": [
        "# Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxKDpZ_3wcuZ",
        "outputId": "c07a1069-b2d3-4dbc-9d90-1db6e2c694f5"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "print(keras.__version__)\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIrkLYK3erBM"
      },
      "outputs": [],
      "source": [
        "model.save('LSTM-Flirt.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA9dXMMvgfUn",
        "outputId": "859a6839-3ae7-49ca-eebf-decb9d117b02"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "new_model = keras.models.load_model('LSTM-Flirt.keras')\n",
        "\n",
        "# Show the model architecture\n",
        "new_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "gGWyKIhnkhJk",
        "outputId": "be2a1152-2ff9-4d53-a9ab-677871084f9f"
      },
      "outputs": [],
      "source": [
        "print(train_dataset_final)\n",
        "tracker = EmissionsTracker()\n",
        "tracker.start()\n",
        "for i in range(4000):\n",
        "    text = train_dataset_final['texts'][i%3000]\n",
        "    predictions = new_model.predict(np.array([sample_text]))\n",
        "emissions: float = tracker.stop()\n",
        "print(f\"Emissions for 2000 runs: \", emissions)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "rn7N0mwwmQrL",
        "outputId": "689b585c-3633-4751-f8b0-54fa25530f5e"
      },
      "outputs": [],
      "source": [
        "emissions_data = tracker.get_emissions()\n",
        "print(tracker.total_emissions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vC617lZWho7V",
        "outputId": "c2f857bf-6e4e-4218-a82e-27da2d95588e"
      },
      "outputs": [],
      "source": [
        "new_model.evaluate(test_ds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hYJq23RCXUdh",
        "V6P1C6u4zTAw",
        "egW9ZW6e5fWc"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
